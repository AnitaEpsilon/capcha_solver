{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_and_save_image(filename: str, input_image_path: str, output_folder_path: str, save_to_tmp: Optional[bool] = False) -> str:\n",
    "    \"\"\"\n",
    "    Разделяет изображение на части и сохраняет их в указанную директорию.\n",
    "\n",
    "    Аргументы:\n",
    "    - filename: Имя файла изображения.\n",
    "    - input_image_path: Путь к директории с исходным изображением.\n",
    "    - output_folder_path: Путь к директории, куда будут сохранены части изображения.\n",
    "    - save_to_tmp: Флаг, указывающий на сохранение во временную папку. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Имя папки, в которую были сохранены части изображения.\n",
    "    \"\"\"\n",
    "    # Open the input image\n",
    "    with Image.open(os.path.join(input_image_path, filename)) as img:\n",
    "\n",
    "        # Get the filename and the extension\n",
    "        extension = os.path.basename(filename).split('.')[-1]\n",
    "\n",
    "        if save_to_tmp:\n",
    "            foldername = \"tmp\"\n",
    "        else:\n",
    "            foldername = filename.split('.')[0]\n",
    "\n",
    "        # Define the size of each individual image, assuming they are of equal height\n",
    "        # and the last image occupies the whole width of the original image\n",
    "        width, height = img.size\n",
    "        single_image_height = height // 2  # Divide by 2 because there are 2 rows\n",
    "        single_image_width = width // 5  # Divide by 5 for the images in the first row\n",
    "        \n",
    "        # Create the output directory\n",
    "        output_directory = os.path.join(output_folder_path, foldername)\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        \n",
    "        # Split the images and save them\n",
    "        for i in range(5):  # For the first row\n",
    "            left = i * single_image_width\n",
    "            right = (i + 1) * single_image_width\n",
    "            box = (left, 0, right, single_image_height)\n",
    "            part_img = img.crop(box)\n",
    "            part_img.save(os.path.join(output_directory, f'x_{i+1}.{extension}'))\n",
    "        \n",
    "        # Save the last image\n",
    "        box = (0, single_image_height, width*(2/15), 2 * single_image_height)\n",
    "        part_img = img.crop(box)\n",
    "        part_img.save(os.path.join(output_directory, f'y.{extension}'))\n",
    "\n",
    "        return foldername"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "\n",
    "def open_image_and_visualize(image_path: str, visualize: Optional[bool] = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Открывает изображение по указанному пути и при необходимости визуализирует его.\n",
    "\n",
    "    Аргументы:\n",
    "    - image_path: Путь к изображению.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Массив изображения в формате numpy.ndarray.\n",
    "    \"\"\"\n",
    "    im = cv2.imread(image_path)\n",
    "    if visualize:\n",
    "        plt.imshow(im)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return im\n",
    "\n",
    "\n",
    "def make_scalar_product_mask(im: np.ndarray, visualize: Optional[bool] = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Создает маску изображения на основе скалярного произведения и при необходимости визуализирует её.\n",
    "\n",
    "    Аргументы:\n",
    "    - im: Исходное изображение в формате numpy.ndarray.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Маску изображения в формате numpy.ndarray.\n",
    "    \"\"\"\n",
    "    image = im/255.\n",
    "    unit_vector = np.array([1/np.sqrt(3), 1/np.sqrt(3), 1/np.sqrt(3)])\n",
    "    unit_image = image/np.sqrt(np.tile(np.sum(image*image, axis=2), (3,1,1)).transpose(1,2,0))\n",
    "    new_im = np.sum(unit_image*unit_vector, axis=2) * 255\n",
    "    if visualize:\n",
    "        plt.imshow(new_im)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return new_im\n",
    "\n",
    "\n",
    "def make_binar(im: np.ndarray, threshold: Optional[int] = 240, visualize: Optional[bool] = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Бинаризует изображение по заданному порогу и при необходимости визуализирует его.\n",
    "\n",
    "    Аргументы:\n",
    "    - im: Исходное изображение в формате numpy.ndarray.\n",
    "    - threshold: Порог для бинаризации. По умолчанию 240.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Бинаризованное изображение в формате numpy.ndarray.\n",
    "    \"\"\"\n",
    "    img = deepcopy(im)\n",
    "    img[img!=img] = 255\n",
    "    img[img<threshold] = 0\n",
    "    img[img>=threshold] = 255\n",
    "    if visualize:\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return img\n",
    "\n",
    "\n",
    "def pad_borders(im: np.ndarray, pad_width: Optional[int] = 3, visualize: Optional[bool] = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Добавляет рамку вокруг изображения и при необходимости визуализирует его.\n",
    "\n",
    "    Аргументы:\n",
    "    - im: Исходное изображение в формате numpy.ndarray.\n",
    "    - pad_width: Ширина рамки. По умолчанию 3.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Изображение с рамкой в формате numpy.ndarray.\n",
    "    \"\"\"\n",
    "    im = np.pad(im, 10, 'linear_ramp', end_values=255)\n",
    "    if visualize:\n",
    "        plt.imshow(im, cmap='gray')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return im\n",
    "\n",
    "\n",
    "def smooth_circle_borders(im: np.ndarray, visualize: Optional[bool] = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Сглаживает границы кругов на изображении и при необходимости визуализирует результат.\n",
    "\n",
    "    Аргументы:\n",
    "    - im: Исходное изображение в формате numpy.ndarray.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Изображение с сглаженными границами в формате numpy.ndarray.\n",
    "    \"\"\"    \n",
    "    kernel_size = 3\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    im = cv2.morphologyEx(im, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    im = cv2.morphologyEx(im, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    if visualize:\n",
    "        plt.imshow(im, cmap='gray')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return im\n",
    "\n",
    "\n",
    "def detect_circles(im: np.ndarray, visualize: Optional[bool] = False) -> list:\n",
    "    \"\"\"\n",
    "    Обнаруживает круги на изображении и при необходимости визуализирует результат.\n",
    "\n",
    "    Аргументы:\n",
    "    - im: Исходное изображение в формате numpy.ndarray.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Список центров обнаруженных кругов.\n",
    "    \"\"\"\n",
    "    im = np.uint8(im)\n",
    "    radius_tolerance = 5\n",
    "    min_radius = 20 - radius_tolerance\n",
    "    max_radius = 20 + radius_tolerance\n",
    "\n",
    "    dp = 1.5  # Обратное отношение разрешения накопителя квадратов к разрешению изображения\n",
    "    minDist = 25  # Минимальное расстояние между центрами обнаруженных кругов\n",
    "    param1 = 40  # Более высокий порог из двух, передаваемых в детектор краев Canny\n",
    "    param2 = 18  # Порог аккумулятора для центров кругов на этапе обнаружения\n",
    "\n",
    "    # Используем HoughCircles для обнаружения кругов\n",
    "    detected_circles = cv2.HoughCircles(\n",
    "        im, \n",
    "        cv2.HOUGH_GRADIENT, \n",
    "        dp, \n",
    "        minDist, \n",
    "        param1=param1, \n",
    "        param2=param2, \n",
    "        minRadius=min_radius, \n",
    "        maxRadius=max_radius\n",
    "    )\n",
    "\n",
    "    # Преобразуем параметры кругов a, b и r в целые числа\n",
    "    detected_circles_rounded = np.uint16(np.around(detected_circles))\n",
    "    \n",
    "    # Визуализируем результат на новом изображении\n",
    "    if visualize:\n",
    "        output_image_circles = cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    centers = []\n",
    "    # Рисуем обнаруженные круги\n",
    "    if detected_circles_rounded is not None:\n",
    "        for i in detected_circles_rounded[0, :]:\n",
    "            center = (i[0], i[1])\n",
    "            centers.append(center)\n",
    "\n",
    "            if visualize:\n",
    "                radius = i[2]\n",
    "                # Рисуем внешний круг\n",
    "                cv2.circle(output_image_circles, center, radius, (0, 255, 0), 2)\n",
    "                # Рисуем центр круга\n",
    "                cv2.circle(output_image_circles, center, 2, (0, 0, 255), 3)\n",
    "\n",
    "    if visualize:\n",
    "        plt.imshow(output_image_circles)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    return centers\n",
    "\n",
    "\n",
    "def substract_padding(centers: list, padding: Optional[int] = 10) -> list:\n",
    "    \"\"\"\n",
    "    Корректирует координаты центров кругов, учитывая добавленную рамку.\n",
    "\n",
    "    Аргументы:\n",
    "    - centers: Список кортежей с координатами центров кругов.\n",
    "    - padding: Ширина добавленной рамки. По умолчанию 10.\n",
    "\n",
    "    Возвращает:\n",
    "    - Список скорректированных координат центров кругов.\n",
    "    \"\"\"\n",
    "    new_centers = []\n",
    "    for c in centers:\n",
    "        new_centers.append((c[0]-padding, c[1]-padding))\n",
    "    return new_centers\n",
    "\n",
    "\n",
    "def sort_centers(centers: list, img_path: str, visualize: Optional[bool] = False) -> list:\n",
    "    \"\"\"\n",
    "    Сортирует центры кругов по их расстоянию до шестой точки и при необходимости визуализирует результат.\n",
    "\n",
    "    Аргументы:\n",
    "    - centers: Список кортежей с координатами центров кругов.\n",
    "    - img_path: Путь к изображению.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Список отсортированных координат центров кругов.\n",
    "    \"\"\"\n",
    "    # Define the coordinates of the 6th point\n",
    "    sixth_point = (40, 190)\n",
    "\n",
    "    # Calculate distances from the sixth point to each of the centers\n",
    "    distances = [np.sqrt((x - sixth_point[0])**2 + (y - sixth_point[1])**2) for x, y in centers]\n",
    "\n",
    "    # Pair each center with its distance from the sixth point\n",
    "    center_distances = list(zip(centers, distances))\n",
    "\n",
    "    # Sort the centers by their distance from the sixth point\n",
    "    sorted_centers = sorted(center_distances, key=lambda x: x[1])\n",
    "\n",
    "    # Extract the sorted centers and their order\n",
    "    sorted_centers_only = [center for center, distance in sorted_centers]\n",
    "\n",
    "    if visualize:\n",
    "        # Load the provided image\n",
    "        img = Image.open(img_path)\n",
    "        # Convert to RGB to plot color on top of the original image\n",
    "        img_rgb = img.convert('RGB')\n",
    "        draw = ImageDraw.Draw(img_rgb)\n",
    "\n",
    "        # Draw the sixth point\n",
    "        draw.ellipse((sixth_point[0]-3, sixth_point[1]-3, sixth_point[0]+3, sixth_point[1]+3), fill='blue', outline='blue')\n",
    "\n",
    "        # Draw the centers, lines, and order\n",
    "        for num, (x, y) in enumerate(sorted_centers_only):\n",
    "            # Draw the center\n",
    "            draw.ellipse((x-3, y-3, x+3, y+3), fill='red', outline='red')\n",
    "            # Draw line from the center to the sixth point\n",
    "            draw.line((x, y, sixth_point[0], sixth_point[1]), fill='green', width=1)\n",
    "            # Annotate the order next to the center\n",
    "            draw.text((x+5, y), f'{num+1}', fill='purple',)\n",
    "\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    return sorted_centers_only\n",
    "\n",
    "\n",
    "def define_corners(center: tuple, radius: Optional[int] = 15) -> tuple:\n",
    "    \"\"\"\n",
    "    Определяет координаты углов квадрата вокруг центра круга с заданным радиусом.\n",
    "\n",
    "    Аргументы:\n",
    "    - center: Координаты центра круга.\n",
    "    - radius: Радиус квадрата. По умолчанию 15.\n",
    "\n",
    "    Возвращает:\n",
    "    - Кортеж с координатами углов квадрата.\n",
    "    \"\"\"\n",
    "    left = center[0] - radius\n",
    "    right = center[0] + radius\n",
    "    up = center[1] - radius\n",
    "    down = center[1] + radius\n",
    "    \n",
    "    return (left, up, right, down)\n",
    "\n",
    "\n",
    "def get_last_directory(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Возвращает имя последней директории в указанном пути.\n",
    "\n",
    "    Аргументы:\n",
    "    - path: Путь, из которого нужно извлечь имя последней директории.\n",
    "\n",
    "    Возвращает:\n",
    "    - Имя последней директории в пути.\n",
    "    \"\"\"\n",
    "    directory_path = os.path.dirname(path)\n",
    "    return os.path.basename(directory_path)\n",
    "\n",
    "\n",
    "\n",
    "def crop_squares(image_path: str, squares: list, output_dir: str, visualize: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    Вырезает квадраты из изображения по заданным координатам и сохраняет их.\n",
    "\n",
    "    Аргументы:\n",
    "    - image_path: Путь к изображению.\n",
    "    - squares: Список кортежей, каждый из которых представляет координаты квадрата.\n",
    "    - output_dir: Директория для сохранения вырезанных квадратов.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "    \"\"\"\n",
    "\n",
    "    subfoldername = os.path.basename(image_path).split('.')[0]\n",
    "    foldername = get_last_directory(image_path)\n",
    "    full_path = os.path.join(output_dir,foldername,subfoldername)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to RGB if it's not already\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    if visualize:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw each square\n",
    "    for i, (left, up, right, down) in enumerate(squares):\n",
    "\n",
    "        if visualize:\n",
    "            # Draw a rectangle for each square\n",
    "            draw.rectangle([(left, up), (right, down)], outline='yellow', width=2)\n",
    "\n",
    "        crop = img.crop((left, up, right, down))\n",
    "        crop_path = os.path.join(full_path, f\"{i+1}.jpg\")\n",
    "        crop.save(crop_path)\n",
    "    \n",
    "    if visualize:\n",
    "        plt.imshow(img)\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def combine_all_preprocessing_functions(image_path, output_dir, visualize=False):\\n    im = open_image_and_visualize(image_path, visualize=visualize)\\n    im = make_scalar_product_mask(im, visualize=visualize)\\n    im = make_binar(im, threshold=240)\\n    im = pad_borders(im, pad_width=10, visualize=visualize)\\n    im = smooth_circle_borders(im, visualize=visualize)\\n    centers = detect_circles(im, visualize=visualize)\\n    centers = substract_padding(centers, padding=10)\\n    centers = sort_centers(centers, image_path, visualize=visualize)\\n    squares = [define_corners(center, radius=20) for center in centers]\\n    crop_squares(image_path, squares, output_dir, visualize=visualize)\\n    return centers'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def combine_all_preprocessing_functions(image_path, output_dir, visualize=False):\n",
    "    im = open_image_and_visualize(image_path, visualize=visualize)\n",
    "    im = make_scalar_product_mask(im, visualize=visualize)\n",
    "    im = make_binar(im, threshold=240)\n",
    "    im = pad_borders(im, pad_width=10, visualize=visualize)\n",
    "    im = smooth_circle_borders(im, visualize=visualize)\n",
    "    centers = detect_circles(im, visualize=visualize)\n",
    "    centers = substract_padding(centers, padding=10)\n",
    "    centers = sort_centers(centers, image_path, visualize=visualize)\n",
    "    squares = [define_corners(center, radius=20) for center in centers]\n",
    "    crop_squares(image_path, squares, output_dir, visualize=visualize)\n",
    "    return centers\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_functions_to_get_centers(image_path: str, visualize: Optional[bool] = False) -> list:\n",
    "    \"\"\"\n",
    "    Комбинирует несколько функций для обработки изображения и получения центров кругов.\n",
    "\n",
    "    Аргументы:\n",
    "    - image_path: Путь к изображению.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "\n",
    "    Возвращает:\n",
    "    - Список координат центров кругов.\n",
    "    \"\"\"\n",
    "    im = open_image_and_visualize(image_path, visualize=visualize)\n",
    "    im = make_scalar_product_mask(im, visualize=visualize)\n",
    "    im = make_binar(im, threshold=240)\n",
    "    im = pad_borders(im, pad_width=10, visualize=visualize)\n",
    "    im = smooth_circle_borders(im, visualize=visualize)\n",
    "    centers = detect_circles(im, visualize=visualize)\n",
    "    centers = substract_padding(centers, padding=10)\n",
    "    centers = sort_centers(centers, image_path, visualize=visualize)\n",
    "    return centers\n",
    "\n",
    "def save_squares(image_path: str, output_dir: str, centers: list, visualize: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    Сохраняет квадраты, вырезанные вокруг центров кругов, в указанную директорию.\n",
    "\n",
    "    Аргументы:\n",
    "    - image_path: Путь к изображению.\n",
    "    - output_dir: Директория для сохранения вырезанных квадратов.\n",
    "    - centers: Список координат центров кругов.\n",
    "    - visualize: Флаг, определяющий необходимость визуализации. По умолчанию False.\n",
    "    \"\"\"\n",
    "    squares = [define_corners(center, radius=20) for center in centers]\n",
    "    crop_squares(image_path, squares, output_dir, visualize=visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install ftfy regex tqdm\n",
    "#! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(base_image: Any, compare_image: Any, similarity: float, i: int) -> None:\n",
    "    \"\"\"\n",
    "    Отображает сравнение базового изображения с изображением для сравнения, включая показатель сходства.\n",
    "\n",
    "    Аргументы:\n",
    "    - base_image: Изображение для отображения как базовое.\n",
    "    - compare_image: Изображение для сравнения с базовым.\n",
    "    - similarity: Показатель сходства между изображениями.\n",
    "    - i: Индекс изображения для сравнения.\n",
    "\n",
    "    Возвращает:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(base_image)\n",
    "    plt.title(\"Base Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(compare_image)\n",
    "    plt.title(f\"Compare to Image {i}\\nSimilarity: {similarity.item():.4f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# Создание объекта reader\n",
    "reader = easyocr.Reader(['en'], gpu=False,)  # Укажите 'gpu=True', если доступна GPU\n",
    "\n",
    "# Функция для распознавания текста с изображения\n",
    "def recognize_digit(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Распознает цифру на изображении с использованием EasyOCR.\n",
    "\n",
    "    Аргументы:\n",
    "    - image_path: Путь к изображению для распознавания.\n",
    "\n",
    "    Возвращает:\n",
    "    - Распознанную цифру в виде строки. В случае отсутствия распознавания возвращает \"1\".\n",
    "    \"\"\"\n",
    "    result = reader.readtext(image_path, detail=1, paragraph=False, allowlist='12345')\n",
    "    #print(\"result:\", result)\n",
    "    if not result:\n",
    "        print(\"None is replaced with 1\")\n",
    "        return \"1\"\n",
    "    for detection in result:\n",
    "        bbox, text, confidence = detection\n",
    "        #print(text, type(text), type(text[0])) \n",
    "        return text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottom_half(input_folder: str, output_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Вырезает и сохраняет нижнюю половину изображения 'y.jpg' из входной папки в выходную папку под именем 'im.jpg'.\n",
    "\n",
    "    Аргументы:\n",
    "    - input_folder: Папка, содержащая исходное изображение 'y.jpg'.\n",
    "    - output_folder: Папка для сохранения результата.\n",
    "\n",
    "    Возвращает:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Open the input image\n",
    "    input_image_path = os.path.join(input_folder, \"y.jpg\")\n",
    "    output_image_path = os.path.join(output_folder, \"im.jpg\")\n",
    "\n",
    "    with Image.open(input_image_path) as img:\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Calculate the box for the bottom half\n",
    "        # The box is defined by (left, upper, right, lower) edges.\n",
    "        box = (0, height // 2, width, height)\n",
    "        \n",
    "        # Crop the bottom half\n",
    "        bottom_half = img.crop(box)\n",
    "        \n",
    "        # Save the cropped image to the specified output path\n",
    "        bottom_half.save(output_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def find_matching_square(path_to_json: str) -> list:\n",
    "    \"\"\"\n",
    "    Находит координаты прямоугольника, содержащего знак плюс, в JSON файле аннотаций.\n",
    "\n",
    "    Аргументы:\n",
    "    - path_to_json: Путь к JSON файлу с аннотациями.\n",
    "\n",
    "    Возвращает:\n",
    "    - Координаты прямоугольника, содержащего знак плюс, в формате списка из двух точек.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path_to_json) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    # First, let's find the coordinates of all pluses\n",
    "    plus_signs = [shape['points'][0] for shape in data['shapes'] if shape['label'] == '+']\n",
    "\n",
    "    # Now, we will check which rectangles contain these plus signs\n",
    "    # A point is inside a rectangle if its x coordinate is between the x coordinates of the rectangle\n",
    "    # and its y coordinate is between the y coordinates of the rectangle\n",
    "\n",
    "    def is_point_in_rect(point, rect):\n",
    "        # unpack points\n",
    "        px, py = point\n",
    "        (rx1, ry1), (rx2, ry2) = rect\n",
    "        return rx1 <= px <= rx2 and ry1 <= py <= ry2\n",
    "\n",
    "    # List to store rectangles that contain a plus\n",
    "    rectangles_with_plus = []\n",
    "\n",
    "    # Iterate over the shapes to find rectangles that contain a plus\n",
    "    for shape in data['shapes']:\n",
    "        if shape['shape_type'] == 'rectangle':\n",
    "            # Get rectangle coordinates\n",
    "            rect_coords = shape['points']\n",
    "            # Check each plus sign\n",
    "            for plus in plus_signs:\n",
    "                if is_point_in_rect(plus, rect_coords):\n",
    "                    return rect_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_point_and_rect(json_file_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Находит координаты точки и прямоугольника, соответствующие условиям задачи, основываясь на данных из JSON файла.\n",
    "\n",
    "    Аргументы:\n",
    "    - json_file_path: Путь к JSON файлу с аннотациями.\n",
    "\n",
    "    Возвращает:\n",
    "    - Кортеж, содержащий координаты точки, номер точки, координаты прямоугольника и смещение прямоугольника по оси X.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from math import sqrt\n",
    "\n",
    "    # Загрузка данных из файла\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Извлечение специальной цифры из лейблов\n",
    "    special_number = int(next(filter(str.isdigit, (shape['label'] for shape in data['shapes']))))\n",
    "\n",
    "    ordinary_points = [shape for shape in data['shapes'] if shape['label'] == '+']\n",
    "    icons = [shape for shape in data['shapes'] if shape['label'] == 'icon']\n",
    "    point_a = (40, 190)  # Главная точка A\n",
    "    \n",
    "    # Вычисление условного расстояния и картиночного номера\n",
    "    def conditional_distance(point_a, point_b):\n",
    "        x_b_mod = point_b[0] % 200\n",
    "        distance = sqrt((x_b_mod - point_a[0]) ** 2 + (point_b[1] - point_a[1]) ** 2)\n",
    "        return distance\n",
    "\n",
    "    ordinary_points_info = [{\n",
    "        'coords': shape['points'][0],\n",
    "        'distance': conditional_distance(point_a, shape['points'][0]),\n",
    "        'picture_number': int(shape['points'][0][0] // 200) + 1\n",
    "    } for shape in ordinary_points]\n",
    "\n",
    "    ordinary_points_info.sort(key=lambda x: x['distance'])\n",
    "    \n",
    "    for index, point in enumerate(ordinary_points_info, start=1):\n",
    "        point['length_number'] = index\n",
    "    \n",
    "    # Находим точку, у которой длинностный номер равен специальной цифре\n",
    "    matching_point = next((p for p in ordinary_points_info if p['length_number'] == special_number), None)\n",
    "    \n",
    "    # Проверка и смещение прямоугольника\n",
    "    def is_point_in_rect(point, rect):\n",
    "        return rect[0][0] <= point[0] <= rect[1][0] and rect[0][1] <= point[1] <= rect[1][1]\n",
    "\n",
    "    def find_shift_and_rect(point, icons):\n",
    "        for icon in icons:\n",
    "            for shift_x in range(0, 10001, 200):  # Предполагаем, что смещение может быть до 10000 пикселей\n",
    "                shifted_rect = [[x + shift_x, y] for x, y in icon['points']]\n",
    "                if is_point_in_rect(point, shifted_rect):\n",
    "                    return shifted_rect, shift_x\n",
    "        return None, None\n",
    "\n",
    "    # Находим подходящий прямоугольник и требуемое смещение\n",
    "    matching_rect, shift_x = find_shift_and_rect(matching_point['coords'], icons)\n",
    "\n",
    "    return matching_point['coords'], matching_point['length_number'], matching_rect, shift_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make benchmarked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def save_correct_answer(rectangle_coords: list, image_path: str, cropped_image_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Вырезает и сохраняет указанный прямоугольник из изображения.\n",
    "\n",
    "    Аргументы:\n",
    "    - rectangle_coords: Координаты прямоугольника для вырезки.\n",
    "    - image_path: Путь к исходному изображению.\n",
    "    - cropped_image_path: Путь для сохранения вырезанного прямоугольника.\n",
    "\n",
    "    Возвращает:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Reload the original image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Define the coordinates of the rectangle to be cropped\n",
    "    #rectangle_coords = [[[77.76795580110496, 47.933701657458556], [123.20441988950277, 95.58011049723757]]]\n",
    "    top_left_x, top_left_y = rectangle_coords[0]\n",
    "    bottom_right_x, bottom_right_y = rectangle_coords[1]\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((top_left_x, top_left_y, bottom_right_x, bottom_right_y))\n",
    "\n",
    "    # Save the cropped image to a file\n",
    "    cropped_image.save(cropped_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 970/970 [00:16<00:00, 60.14it/s]\n"
     ]
    }
   ],
   "source": [
    "input_image_path = \"data/raw\"\n",
    "output_folder_path = \"data/interim/orbits\"\n",
    "\n",
    "files = os.listdir(input_image_path)\n",
    "jpg_files = [file for file in files if file.endswith('.jpg')]\n",
    "len(jpg_files)\n",
    "\n",
    "for file in tqdm(jpg_files):\n",
    "    foldername = split_and_save_image(file, input_image_path, output_folder_path, save_to_tmp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 176/970 [00:02<00:09, 83.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 221/970 [00:02<00:07, 98.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 789/970 [00:08<00:01, 109.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185974\n",
      "185996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 811/970 [00:08<00:01, 104.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186056\n",
      "186098\n",
      "186121\n",
      "186130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 846/970 [00:08<00:00, 137.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186159\n",
      "186162\n",
      "186172\n",
      "186174\n",
      "186176\n",
      "186193\n",
      "186198\n",
      "186207\n",
      "186209\n",
      "186229\n",
      "186232\n",
      "186242\n",
      "186245\n",
      "186269\n",
      "186271\n",
      "186277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 875/970 [00:09<00:01, 86.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186288\n",
      "186305\n",
      "186317\n",
      "186318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 897/970 [00:09<00:00, 91.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186357\n",
      "186361\n",
      "186390\n",
      "186422\n",
      "186424\n",
      "186426\n",
      "186450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 936/970 [00:09<00:00, 129.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186462\n",
      "186464\n",
      "186467\n",
      "186492\n",
      "186494\n",
      "186497\n",
      "186519\n",
      "186522\n",
      "186524\n",
      "186525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 951/970 [00:09<00:00, 99.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186585\n",
      "186596\n",
      "186616\n",
      "186628\n",
      "186637\n",
      "186639\n",
      "186658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 970/970 [00:10<00:00, 96.86it/s] \n"
     ]
    }
   ],
   "source": [
    "input_image_path = \"data/raw\"\n",
    "benchmarked_path = \"data/benchmarked\"\n",
    "parsed_path = \"data/interim/orbits\"\n",
    "\n",
    "\n",
    "os.makedirs(benchmarked_path, exist_ok=True)\n",
    "\n",
    "files = os.listdir(input_image_path)\n",
    "json_files = [file for file in files if file.endswith('.json')]\n",
    "len(json_files)\n",
    "\n",
    "for file in tqdm(json_files):\n",
    "    rectangle_coords = find_matching_square(os.path.join(input_image_path, file))\n",
    "    #print(square)\n",
    "    foldername = file.split(\".\")[0]\n",
    "    os.makedirs(os.path.join(benchmarked_path, foldername), exist_ok=True)\n",
    "    try:\n",
    "        save_correct_answer(rectangle_coords, \n",
    "                            image_path=os.path.join(input_image_path, f\"{foldername}.jpg\"), \n",
    "                            cropped_image_path=os.path.join(benchmarked_path, foldername, \"correct.jpg\"))\n",
    "\n",
    "        save_bottom_half(input_folder=os.path.join(parsed_path, foldername),\n",
    "                        output_folder=os.path.join(benchmarked_path, foldername))\n",
    "    except:\n",
    "        print(foldername)\n",
    "\n",
    "                                 \n",
    "#for file in tqdm(jpg_files[:5]):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_for_answer(centers: list, idx: int, digit: int) -> list:\n",
    "    \"\"\"\n",
    "    Рассчитывает центр для ответа на основе индекса и распознанной цифры.\n",
    "\n",
    "    Аргументы:\n",
    "    - centers: Список центров.\n",
    "    - idx: Индекс выбранного изображения.\n",
    "    - digit: Распознанная цифра.\n",
    "\n",
    "    Возвращает:\n",
    "    - Центр для ответа в виде списка координат.\n",
    "    \"\"\"\n",
    "    width = 200\n",
    "    center = list(centers[digit-1])\n",
    "    center[0] = center[0] + (idx-1) * width\n",
    "    return center\n",
    "\n",
    "\n",
    "def is_point_in_rectangle(point: tuple, rectangle: tuple) -> bool:\n",
    "    \"\"\"\n",
    "    Проверяет, находится ли точка внутри заданного прямоугольника.\n",
    "\n",
    "    Аргументы:\n",
    "    - point: Координаты точки (x, y).\n",
    "    - rectangle: Координаты прямоугольника ((x1, y1), (x2, y2)).\n",
    "\n",
    "    Возвращает:\n",
    "    - True, если точка находится внутри прямоугольника, иначе False.\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    x1, y1 = rectangle[0]\n",
    "    x2, y2 = rectangle[1]\n",
    "    \n",
    "    if (x >= x1 and x <= x2) and (y >= y1 and y <= y2):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None is replaced with 1\n",
      "SOME ERROR: 183121.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:00<00:00,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOME ERROR: 183122.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOME ERROR: 183125.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_image_path = \"data/raw\"\n",
    "output_folder_path = \"data/interim/orbits\"\n",
    "output_for_objects_path = \"data/interim/objects\"\n",
    "\n",
    "files = os.listdir(input_image_path)\n",
    "jpg_files = [file for file in files if file.endswith('.jpg')]\n",
    "#len(jpg_files)\n",
    "\n",
    "for file in tqdm(jpg_files[4:7]):\n",
    "    try:\n",
    "        foldername = split_and_save_image(file, input_image_path, output_folder_path, save_to_tmp=True)\n",
    "        name_without_format = file.split(\".\")[0]\n",
    "        digit = recognize_digit(os.path.join(output_folder_path, foldername, \"y.jpg\"))\n",
    "        #print(file, digit)\n",
    "        save_bottom_half(input_folder=os.path.join(output_folder_path, foldername),\n",
    "                        output_folder=os.path.join(output_for_objects_path, foldername))\n",
    "        list_of_centers = []\n",
    "        for i in range(1, 6):\n",
    "            ##print(\"AAAAAA: \", i, file)\n",
    "            image_path = os.path.join(output_folder_path, foldername, f\"x_{i}.jpg\")\n",
    "            centers = combine_all_preprocessing_functions(image_path, \n",
    "                                                        output_for_objects_path, \n",
    "                                                        visualize=False)\n",
    "            list_of_centers.append(centers)\n",
    "\n",
    "        idx = compare_images(base_image_folder=os.path.join(output_for_objects_path, foldername), \n",
    "                            compare_image_folder=os.path.join(output_for_objects_path, foldername), \n",
    "                            digit=digit, \n",
    "                            model=model, \n",
    "                            preprocess=preprocess,\n",
    "                            visualize=False,)\n",
    "        centers = list_of_centers[idx-1]\n",
    "        matching_point, number, correct_square, shift_x = find_point_and_rect(json_file_path=os.path.join(input_image_path, f\"{name_without_format}.json\"))\n",
    "        answer =  center_for_answer(centers, idx, int(digit))\n",
    "        #print(answer)\n",
    "        print(file)\n",
    "        print(answer, correct_square, is_point_in_rectangle(point=answer, rectangle=correct_square))\n",
    "    #print(idx, answer, correct_square)\n",
    "    #print(\"All centers:\", centers)\n",
    "        #draw_squares(image_path, squares, output_dir, visualize=False, save_to_tmp=True)\n",
    "    except:\n",
    "        print(\"SOME ERROR:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.2)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy)\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\epsil\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.4 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 30.7/53.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 53.4/53.4 kB 685.0 kB/s eta 0:00:00\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, ftfy\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "Successfully installed ftfy-6.1.3 wcwidth-0.2.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\epsil\\AppData\\Local\\Temp\\pip-req-build-aovygaet'\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\epsil\\appdata\\local\\temp\\pip-req-build-aovygaet\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (6.1.3)\n",
      "Requirement already satisfied: regex in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: torch in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (2.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (0.17.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->clip==1.0) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->clip==1.0) (10.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\epsil\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\epsil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (pyproject.toml): started\n",
      "  Building wheel for clip (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369580 sha256=3486c1dc136bb5e7e882c666ef21836c59a96108f50aa4817cb4142d9bcd856e\n",
      "  Stored in directory: C:\\Users\\epsil\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-8a2t5hx4\\wheels\\3f\\7c\\a4\\9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epsil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clip\\clip.py:57: UserWarning: C:\\Users\\epsil/.cache/clip\\ViT-B-32.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "100%|████████████████████████████████████████| 338M/338M [07:45<00:00, 761kiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "# Загрузка предварительно обученной модели CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "class CLIPFineTuneModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для настройки модели CLIP с добавлением линейного слоя для финетюнинга.\n",
    "\n",
    "    Атрибуты:\n",
    "    - clip_model (torch.nn.Module): предобученная модель CLIP.\n",
    "    - linear (torch.nn.Linear): линейный слой для трансформации признаков.\n",
    "    - cosine_similarity (torch.nn.CosineSimilarity): слой для вычисления косинусного сходства между векторами признаков.\n",
    "\n",
    "    Методы:\n",
    "    - forward(img1, img2): Проход вперед, который принимает пары изображений и возвращает оценку их сходства.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clip_model):\n",
    "        \"\"\"\n",
    "        Инициализирует класс с предобученной моделью CLIP и добавляет линейный слой.\n",
    "\n",
    "        Параметры:\n",
    "        - clip_model (torch.nn.Module): предобученная модель CLIP.\n",
    "        \"\"\"\n",
    "        super(CLIPFineTuneModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.linear = nn.Linear(512, 64)\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        \"\"\"\n",
    "        Выполняет проход вперед, принимая два изображения и вычисляя их сходство.\n",
    "\n",
    "        Параметры:\n",
    "        - img1 (torch.Tensor): тензор первого изображения.\n",
    "        - img2 (torch.Tensor): тензор второго изображения.\n",
    "\n",
    "        Выходные данные:\n",
    "        - similarity (torch.Tensor): тензор с оценками сходства изображений.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            features1 = self.clip_model.encode_image(img1).to(dtype=torch.float32)\n",
    "            features2 = self.clip_model.encode_image(img2).to(dtype=torch.float32)\n",
    "\n",
    "        transformed_features1 = self.linear(features1).to(dtype=torch.float32)\n",
    "        transformed_features2 = self.linear(features2).to(dtype=torch.float32)\n",
    "        transformed_features1 = transformed_features1 / transformed_features1.norm(dim=-1, keepdim=True)\n",
    "        transformed_features2 = transformed_features2 / transformed_features2.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = self.cosine_similarity(transformed_features1, transformed_features2).unsqueeze(-1).to(dtype=torch.float32)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPFineTuneModel(\n",
       "  (clip_model): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (cosine_similarity): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация модели надстройки\n",
    "finetune_model = CLIPFineTuneModel(model).to(device)\n",
    "\n",
    "# Путь к сохраненной модели\n",
    "model_path = 'finetuned_clip_model_roc093.pth'\n",
    "\n",
    "# Загрузка весов в модель\n",
    "#finetune_model.load_state_dict(torch.load(model_path))\n",
    "finetune_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "finetune_model.eval()  # Переключение модели в режим оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_image_features(image_path, model, preprocess):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_processed = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(image_processed).to(dtype=torch.float32)\n",
    "    return features, image\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Сравнение изображений и вывод результатов\n",
    "\"\"\"def compare_images(base_image_folder, compare_image_folder, digit, finetune_model, preprocess, visualize=False):\n",
    "    base_image_path = os.path.join(base_image_folder, \"im.jpg\")\n",
    "    base_features, base_image = get_image_features(base_image_path, model, preprocess)\n",
    "    \n",
    "    max_similarity = -1  # Инициализируем переменную для сохранения наибольшей схожести\n",
    "    max_similarity_index = -1  # Инициализируем переменную для сохранения индекса наибольшей схожести\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        compare_image_path = os.path.join(compare_image_folder, f\"x_{i}\", f\"{digit}.jpg\")\n",
    "        try:\n",
    "            #print(\"PATH: \", compare_image_path)\n",
    "            compare_features, compare_image = get_image_features(compare_image_path, model, preprocess)\n",
    "            similarity = (base_features @ compare_features.T).cpu().numpy()\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                max_similarity_index = i\n",
    "            if visualize:\n",
    "                display_comparison(base_image, compare_image, similarity, i)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Файл {compare_image_path} не найден.\")\n",
    "\n",
    "    return max_similarity_index  # Возвращает индекс с наибольшей схожестью\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def compare_images(base_image_folder: str, compare_image_folder: str, digit: int, finetune_model, preprocess, visualize: bool = False) -> int:\n",
    "    \"\"\"\n",
    "    Функция для сравнения изображений и определения наиболее похожего.\n",
    "    \n",
    "    :param base_image_folder: Путь к папке с базовым изображением.\n",
    "    :param compare_image_folder: Путь к папке с изображениями для сравнения.\n",
    "    :param digit: Цифра для определения номера сравниваемого изображения.\n",
    "    :param finetune_model: Модель для сравнения изображений.\n",
    "    :param preprocess: Функция предварительной обработки изображений.\n",
    "    :param visualize: Флаг для визуализации процесса сравнения (по умолчанию False).\n",
    "    \n",
    "    :return: Индекс наиболее похожего изображения.\n",
    "    \"\"\"\n",
    "    base_image_path = os.path.join(base_image_folder, \"im.jpg\")\n",
    "    #base_features, base_image = get_image_features(base_image_path, clip_model, preprocess)\n",
    "    base_image_raw = Image.open(base_image_path).convert(\"RGB\")\n",
    "    base_image = preprocess(base_image_raw).unsqueeze(0).to(device)\n",
    "\n",
    "    max_similarity = -1\n",
    "    max_similarity_index = -1\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        compare_image_path = os.path.join(compare_image_folder, f\"x_{i}\", f\"{digit}.jpg\")\n",
    "        compare_image_raw = Image.open(compare_image_path).convert(\"RGB\")\n",
    "        compare_image = preprocess(compare_image_raw).unsqueeze(0).to(device)\n",
    "        try:\n",
    "            #compare_features, compare_image = get_image_features(compare_image_path, clip_model, preprocess)\n",
    "            # Использование модели надстройки для сравнения\n",
    "            similarity = finetune_model(base_image, compare_image) #base_features, compare_features).cpu().numpy()\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                max_similarity_index = i\n",
    "            if visualize:\n",
    "                display_comparison(base_image_raw, compare_image_raw, similarity, i)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Файл {compare_image_path} не найден.\")\n",
    "\n",
    "    return max_similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(values: List[float]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Вычисляет медиану списка значений.\n",
    "\n",
    "    Аргументы:\n",
    "    - values: Список значений.\n",
    "\n",
    "    Возвращает:\n",
    "    - Медианное значение списка. Возвращает None, если список пуст.\n",
    "    \"\"\"\n",
    "    sorted_values = sorted(values)\n",
    "    n = len(sorted_values)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    if n % 2 == 1:\n",
    "        return sorted_values[n // 2]\n",
    "    else:\n",
    "        return (sorted_values[n // 2 - 1] + sorted_values[n // 2]) / 2\n",
    "\n",
    "def calculate_median_coordinates(list_of_lists: List[List[tuple]]) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Вычисляет медианные координаты для списка списков координат.\n",
    "\n",
    "    Аргументы:\n",
    "    - list_of_lists: Список списков координат.\n",
    "\n",
    "    Возвращает:\n",
    "    - Список медианных координат.\n",
    "    \"\"\"\n",
    "    median_coordinates = []\n",
    "    for i in range(5):  # Предполагаем, что требуется 5 пар координат\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        for lst in list_of_lists:\n",
    "            if i < len(lst):  # Убедимся, что индекс не выходит за пределы списка\n",
    "                x_values.append(lst[i][0])\n",
    "                y_values.append(lst[i][1])\n",
    "        median_x = median(x_values)\n",
    "        median_y = median(y_values)\n",
    "        if median_x and median_y:\n",
    "            median_coordinates.append((median_x, median_y))\n",
    "    return median_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(input_image_path: str = \"data/raw\",\n",
    "                      output_folder_path: str = \"data/interim/orbits\",\n",
    "                      output_for_objects_path: str = \"data/interim/objects\",\n",
    "                      path_to_chosen_files: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Вычисляет метрики для набора изображений.\n",
    "\n",
    "    Аргументы:\n",
    "    - input_image_path: Путь к входным изображениям.\n",
    "    - output_folder_path: Путь к промежуточным результатам.\n",
    "    - output_for_objects_path: Путь к обработанным объектам.\n",
    "    - path_to_chosen_files: Путь к файлу со списком выбранных файлов для обработки.\n",
    "\n",
    "    Возвращает:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    is_correct = []\n",
    "    files = os.listdir(input_image_path)\n",
    "\n",
    "    if path_to_chosen_files:\n",
    "        with open(path_to_chosen_files, 'r') as file:\n",
    "            valid_names = file.read().split(\"\\n\")\n",
    "            jpg_files = [file for file in files if file.split('.')[0] in valid_names and file.endswith('.jpg')]\n",
    "            print(valid_names)\n",
    "    else:\n",
    "        jpg_files = [file for file in files if file.endswith('.jpg')]\n",
    "\n",
    "\n",
    "    for file in tqdm(jpg_files[:10]):\n",
    "    #for file in tqdm(errors[:3]):\n",
    "        #try:\n",
    "        foldername = split_and_save_image(file, input_image_path, output_folder_path, save_to_tmp=True)\n",
    "        name_without_format = file.split(\".\")[0]\n",
    "        digit = recognize_digit(os.path.join(output_folder_path, foldername, \"y.jpg\"))\n",
    "        save_bottom_half(input_folder=os.path.join(output_folder_path, foldername),\n",
    "                        output_folder=os.path.join(output_for_objects_path, foldername))\n",
    "        list_of_centers = []\n",
    "        for i in range(1, 6):\n",
    "            image_path = os.path.join(output_folder_path, foldername, f\"x_{i}.jpg\")\n",
    "            centers = combine_functions_to_get_centers(image_path, visualize=False)\n",
    "            list_of_centers.append(centers)\n",
    "\n",
    "        centers = calculate_median_coordinates(list_of_centers)\n",
    "        for i in range(1, 6):\n",
    "            image_path = os.path.join(output_folder_path, foldername, f\"x_{i}.jpg\")\n",
    "            save_squares(image_path, output_for_objects_path, centers, visualize=True)\n",
    "        \n",
    "        \"\"\"for i in range(1, 6):\n",
    "            image_path = os.path.join(output_folder_path, foldername, f\"x_{i}.jpg\")\n",
    "            centers = combine_all_preprocessing_functions(image_path, \n",
    "                                                        output_for_objects_path, \n",
    "                                                        visualize=False)\n",
    "            list_of_centers.append(centers)\"\"\"\n",
    "        \n",
    "\n",
    "        idx = compare_images(base_image_folder=os.path.join(output_for_objects_path, foldername), \n",
    "                            compare_image_folder=os.path.join(output_for_objects_path, foldername), \n",
    "                            digit=digit, \n",
    "                            finetune_model=finetune_model, \n",
    "                            preprocess=preprocess,\n",
    "                            visualize=True)        \n",
    "        #centers = calculate_median_coordinates(list_of_centers)\n",
    "\n",
    "       \n",
    "        \n",
    "        print(\"LIST OF CENTRERS:\", list_of_centers)\n",
    "        print(\"CENTERS: \", centers)\n",
    "        print(\"IDX:\", idx, \"DIGIT:\", digit)\n",
    "        matching_point, number, correct_square, shift_x = find_point_and_rect(json_file_path=os.path.join(input_image_path, f\"{name_without_format}.json\"))\n",
    "        answer =  center_for_answer(centers, idx, int(digit))\n",
    "        print(\"ANSWER:\", answer, \"correct_square:\", correct_square, \"shift_x:\", shift_x)\n",
    "        is_ok = int(is_point_in_rectangle(point=answer, rectangle=correct_square))\n",
    "        \n",
    "        if not is_ok:\n",
    "            print(file)\n",
    "        is_correct.append(is_ok)\n",
    "        #print(file)\n",
    "        #print(answer, correct_square, is_point_in_rectangle(point=answer, rectangle=correct_square))\n",
    "    #except:\n",
    "        #is_correct.append(0)\n",
    "        #print(\"SOME ERROR:\", file)\n",
    "        \n",
    "    print(f\"Number of evaluated samples: {len(is_correct)}\")\n",
    "    print(f\"Accuracy: {sum(is_correct)/len(is_correct)}\")\n",
    "    print(is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['185744', '183271', '184790', '183166', '183965', '184281', '186433', '183131', '183415', '184230', '185242', '186498', '183751', '183474', '183487', '185946', '185003', '183959', '183610', '185771', '183255', '186137', '185245', '183629', '184824', '183641', '183257', '185681', '185894', '185630', '185658', '185141', '184373', '185453', '183235', '183135', '183770', '186414', '184537', '186446', '184640', '183419', '184822', '185492', '186428', '184148', '183225', '183535', '184208', '186507', '184106', '186028', '185524', '184619', '183218', '183187', '185232', '185097', '185596', '185346', '185014', '184183', '185182', '185291', '185536', '186372', '184018', '185896', '184935', '183396', '183660', '185607', '183988', '184236', '185851', '184987', '185352', '185407', '185364', '186350', '184605', '184849', '186377', '185588', '185556', '185717', '186064', '185384', '184993', '185069', '185020', '184921', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/interim/objects\\\\tmp\\\\im.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_chosen_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_files.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 36\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(input_image_path, output_folder_path, output_for_objects_path, path_to_chosen_files)\u001b[0m\n\u001b[0;32m     34\u001b[0m name_without_format \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     35\u001b[0m digit \u001b[38;5;241m=\u001b[39m recognize_digit(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder_path, foldername, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 36\u001b[0m \u001b[43msave_bottom_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoldername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_for_objects_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoldername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m list_of_centers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36msave_bottom_half\u001b[1;34m(input_folder, output_folder)\u001b[0m\n\u001b[0;32m     24\u001b[0m bottom_half \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcrop(box)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Save the cropped image to the specified output path\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mbottom_half\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_image_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\epsil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2410\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2408\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2410\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2413\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/interim/objects\\\\tmp\\\\im.jpg'"
     ]
    }
   ],
   "source": [
    "calculate_metrics(path_to_chosen_files=\"test_files.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(path_to_chosen_files=\"test_files.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [\"183135.jpg\", \"183225.jpg\", \"183255.jpg\", \"183257.jpg\", \"183415.jpg\", \"183474.jpg\", \"183487.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['183099.jpg', '183103.jpg', '183108.jpg', '183118.jpg', '183121.jpg']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpg_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'orbits/train\\\\183125.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m tqdm(filenames):\n\u001b[1;32m---> 12\u001b[0m     \u001b[43msplit_and_save_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to_tmp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36msplit_and_save_image\u001b[1;34m(filename, input_image_path, output_folder_path, save_to_tmp)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mРазделяет изображение на части и сохраняет их в указанную директорию.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m- Имя папки, в которую были сохранены части изображения.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Open the input image\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Get the filename and the extension\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     extension \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(filename)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_to_tmp:\n",
      "File \u001b[1;32mc:\\Users\\epsil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'orbits/train\\\\183125.jpg'"
     ]
    }
   ],
   "source": [
    "# Define the paths\n",
    "input_image_path = 'orbits/train'\n",
    "output_folder_path = 'orbits/parsed_input'\n",
    "\n",
    "#\n",
    "filenames = [\"183199.jpg\", \"183201.jpg\", \"183203.jpg\", \"183205.jpg\", \n",
    "\"183208.jpg\", \"183218.jpg\", \"183220.jpg\"]\n",
    "filenames = [\"183125.jpg\", \"183108.jpg\", \"183239.jpg\"]\n",
    "\n",
    "# Call the function\n",
    "for filename in tqdm(filenames):\n",
    "    split_and_save_image(filename, input_image_path, output_folder_path, save_to_tmp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
